{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport csv as csv\nimport random\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\nrandom.seed(42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def fillTableWOint(table,df):\n    with open(df, 'r') as f:\n        reader = csv.reader(f, delimiter=\" \")\n        row = -1\n        for i in reader:\n            row +=1\n            \n            for o in i:\n                data = o.split(\":\")\n                \n                if(int(data[0]) == 0 or int(data[0])==1):\n                    table[row, 0] = int(data[0])\n                else:\n                    table[row, int(data[0])] = data[1]\n\n    return table\n\ndef createNPYWOint(input):\n    print(f\"{input}\")\n    table = newTable(input)\n    fillTableWOint(table,input)\n    print(\"Done\")\n    return table\n\ndef createNPY(input):\n    print(f\"{input}\")\n    table = newTable(input)\n    fillTable(table,input)\n    print(\"Done\")\n    return table\n\ndef fillTable(table,df):\n    with open(df, 'r') as f:\n        reader = csv.reader(f, delimiter=\" \")\n        row = -1\n        for i in reader:\n            row +=1\n            \n            for o in i:\n                data = o.split(\":\")\n                \n                if(int(data[0]) == 0 or int(data[0])==1):\n                    table[row, 0] = int(data[0])\n                else:\n                    table[row, int(data[0])] = int(data[1])\n\n    return table\n\ndef newTable(data):\n    max = 0\n    count = 0\n    \n    with open(data, 'r', newline ='') as f:\n        reader = csv.reader(f, delimiter=' ')\n        for line in reader:\n            count +=1\n            \n            for i in line:\n                index = int(i.split(\":\")[0])\n                \n                if index > max:\n                    max = index\n    return np.zeros((count, max+1))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def create_weights(length):\n  w = np.zeros(length)\n  for i in range(length):\n    w[i] = random.uniform(-0.01,0.01)\n  return w","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_predictions(w,dataset):\n  append = np.ones((dataset.shape[0],1))\n  dataset = np.concatenate((dataset,append),axis = 1)\n  features = dataset[:,1:]\n  #TODO iterate through features and weights to get predictions\n  predictions = np.array([])\n  for i in range(features.shape[0]):\n    \n    prediction = predict(features[i],w)\n    if prediction > 0:\n        predictions = np.append(predictions,1)\n    else:\n      predictions = np.append(predictions,0)\n    #predictions = np.append(predictions, yp)\n  return predictions ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(x,w):\n  return x.dot(w) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def accuracy(dataset, predictions):\n  data = np.copy(dataset)\n  \n  labels = data[:,0:1]\n  correct = 0\n\n  for i in range(len(labels)):\n    if labels[i] == predictions[i]:\n      correct += 1\n\n  return correct / float(len(labels)) * 100.0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def sub_gradient(dataset,epochs,lr,C):\n  \n  #create weight\n  w = create_weights(dataset.shape[1])\n  \n  append = np.ones((dataset.shape[0],1))\n  dataset = np.concatenate((dataset,append),axis = 1)\n\n  for epoch in range(1,epochs):\n    \n    if((epoch/epochs)==0.25):\n        print(\"1/4 way done\")\n    elif ((epoch/epochs)==0.50):\n        print(\"1/2 way done\")\n    elif ((epoch/epochs)==0.75):\n        print(\"3/4 way done\")\n    \n    \n    # shuffling data and then choose\n    dataset_shuffle = np.copy(dataset)\n    np.random.shuffle(dataset_shuffle)\n    features = dataset_shuffle[:,1:]\n    labels = dataset_shuffle[:,0:1]\n    for i in range(features.shape[0]):\n      rate = learning_rate(epochs,lr)\n      \n      if (labels[i,0] == 0):\n        y = -1\n      else:\n        y = 1\n    \n      if (y * np.dot(w.T,features[i])  <= 1):\n        w =(1-rate)*w+(rate*y*C*features[i])\n      else:\n        w = (1-rate)*w\n    #print(epoch)\n  #print(\"done\")\n  return w\n\ndef learning_rate(epochs,lr):\n  y_t = lr/(1+epochs)\n  return y_t","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#test sub gradient \ninput_file_train = '/kaggle/input/uofu-ml-fall-2020/project_data/data/bag-of-words/bow.train.libsvm'\ninput_file_test = '/kaggle/input/uofu-ml-fall-2020/project_data/data/bag-of-words/bow.test.libsvm'\ninput_file_eval = '/kaggle/input/uofu-ml-fall-2020/project_data/data/bag-of-words/bow.eval.anon.libsvm'\n\ntrain = createNPY(input_file_train)\ntest = createNPY(input_file_test)\nevalTable = createNPY(input_file_eval)\npd.DataFrame(train).to_csv(\"/kaggle/working/train.csv\", index=None)\npd.DataFrame(test).to_csv(\"/kaggle/working/test.csv\", index=None)\npd.DataFrame(evalTable).to_csv(\"/kaggle/working/evalTable.csv\", index=None)\n\ntrain = pd.read_csv(\"/kaggle/working/train.csv\",dtype=int,header = None)\ntrain = train.to_numpy()\n\ntest = pd.read_csv(\"/kaggle/working/test.csv\",dtype=int,header = None)\ntest = test.to_numpy()\n\nevalTable = pd.read_csv(\"/kaggle/working/evalTable.csv\",dtype=int,header = None)\nevalTable = evalTable.to_numpy()\nprint(\"Done saving csvs\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"Testing Sub-Gradient SVM...\")\n# y_o=[1,0.1,0.01,0.001,0.0001]\n# trade_off=[1000,100,10,1,0.1,0.01]\n# best_lr=10\n# best_trade_off = 1000\n# best_acc = 0\n# previous_acc=0\n\n# for i in y_o:\n#   for x in trade_off:\n\n      \n    \n#     epochs = 100\n#     w= sub_gradient(train,epochs,i,x)\n#     preds = create_predictions(w,test)\n#     percentage = accuracy(test, preds)\n#     #print(percentage)\n\n\n\n#     print(f\"Learning rate: {i}, trade-off: {x}, accuracy: {percentage}\")\n\n#     if percentage > previous_acc:\n#       best_lr = i\n#       best_trade_off = x\n#       best_acc = percentage\n#       previous_acc = percentage\n\n# print(f\"The best average cross-validation accuracy was {best_acc} with a learning rate of {best_lr} and a trade off value of {best_trade_off}\")\n\nprint(f\"Testing best parameters....\")\n#w= sub_gradient(train,1000,best_lr,best_trade_off)\nw= sub_gradient(train,1000,0.001,100)\npreds = create_predictions(w,test)\npercentage = accuracy(test, preds)\nprint(percentage)\n#pd.DataFrame(preds).to_csv(\"/kaggle/working/testing\", header=[\"example_id\"], index=None)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"preds = create_predictions(w,evalTable)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npreds = np.delete(preds,0)\nprint(preds.shape)\nprint()\nid = '/kaggle/input/uofu-ml-fall-2020/project_data/data/eval.ids'\nexample = np.genfromtxt(id, delimiter = '\\n')\nexample = example.reshape(preds.shape[0],1)\npreds = preds.reshape(preds.shape[0],1)\n\n\noutput = np.concatenate((example, preds), axis = 1).astype(int)\n  \npd.DataFrame(output).to_csv(\"/kaggle/working/bow.eval.subgradient.prediction.csv\", header=[\"example_id\", \"label\"], index=None)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}